\documentclass{exercise}
\usepackage{global-settings}
\usepackage{multicol}
\usepackage{subcaption}
\usepackage[version=4]{mhchem}

\def\modelsolution{1}

\setcounter{tutorial}{8}
\setcounter{exercise}{1}
\release{Mittwoch, 04.12.2024}
\submission{Mittwoch, 11.12.2024, 14 Uhr}


\DeclareMathOperator\artanh{artanh}

\begin{document}

\clearpage
\makeheader

\exercise{Fragen (2P)}
Stellen Sie \emph{pro Person} zwei relevante Fragen zu den Inhalten der Vorlesung \enquote{Einf\"uhrung in die Kern- und Elementarteilchenphysik}.

\exercise{Lorentztransformationen (7P)}

Die Lorentzgruppe ist die Gruppe an Transformationen auf Vierervektoren, unter denen die Minkowskimetrik (die Metrik des flachen Raumes) invariant bleibt:
\begin{equation}
    L := O(3,1) =  \left\{ \Lambda \in \mathbb{R}^{4 \times 4} \, | \, \Lambda_{\phantom{\mu} \mu}^\alpha \, g_{\alpha \beta} \, \Lambda_{\phantom{\nu} \nu}^\beta   = g_{\mu \nu} \right\} \,.
\end{equation} 

Eine wichtige Untergruppe der Lorentztransformationen (LTs) sind die eigentlich-orthochronen Lorentztransformationen
\begin{equation}
    L_+^\uparrow := \left\{ \Lambda \in L \, | \, \det \Lambda = 1 \, \wedge \, \text{sgn} \Lambda^0_{\phantom{0} 0} = 1 \right\} \,,
\end{equation} 
die ausschließlich Boosts und Rotationen umfasst.
Neben den eigentlich-orthochronen LTs existieren drei weitere Komponenten, die zusammen die Lorentzgruppe bilden.
Diese sind über Parit\"at und/oder Zeitumkehr mit der Untergruppe der eigentlich-orthochronen LTs verbunden.

\begin{enumerate}
    \item Geben Sie die Matrixformen der Paritäts- und Zeitumkehrtransformationen $P$ und $T$ an.
    
    \solution{
    \begin{align*}
        P &= \begin{pmatrix}
            1 & 0 & 0 & 0 \\
            0 & -1 & 0 & 0 \\
            0 & 0 & -1 & 0 \\
            0 & 0 & 0 & -1
        \end{pmatrix}
    \end{align*}
    \begin{align*}
        T &= \begin{pmatrix}
            -1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 0 & 0 & 1
        \end{pmatrix}
    \end{align*}
    \textit{1 Punkt}
    }

    \item Zeigen Sie, dass $\det \Lambda = \pm 1$ gilt. 
    \solution{
        Wir starten mit
        \begin{equation}
            \Lambda^T g \Lambda = g \,.
        \end{equation}
        Dann folgt
        \begin{align*}
            \det(\Lambda^T) \det(g) \det(\Lambda) &= \det(g) \\
            \det(\Lambda^T) \det(\Lambda) &= 1 \\
            \det(\Lambda)^2 &= 1 \\
            \det(\Lambda) &= \pm 1 \,.
        \end{align*}
        \textit{1 Punkt}
    }

    \item Betrachten Sie einen Lorentzboost in $x$-Richtung mit
    \begin{equation}
        \label{eq:BoostsMatrix}
        \Lambda = \begin{pmatrix}
            \gamma & -\beta \gamma & 0 & 0 \\
            -\beta \gamma & \gamma & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 0 & 0 & 1
        \end{pmatrix}
    \end{equation}
    und dem bekannten $\gamma = 1/\sqrt{1-\beta^2}$.
    Zeigen Sie, dass $\Lambda \in L_+^\uparrow$ gilt.

    \solution{
        Wir teilen den Beweis in drei Schritte auf:
        \begin{enumerate}
            \item  $\Lambda \in L$
            
            Offensichtlich ist $\Lambda$ eine reelle $4 \times 4$ da $\gamma \,, \beta \in \mathbb{R}$.
            Des Weiteren gilt:
            \begin{align*}
                \Lambda^T g \Lambda &= \begin{pmatrix}
                    \gamma & -\beta \gamma & 0 & 0 \\
                    -\beta \gamma & \gamma & 0 & 0 \\
                    0 & 0 & 1 & 0 \\
                    0 & 0 & 0 & 1
                    \end{pmatrix}^T
                    \begin{pmatrix}
                    1 & 0 & 0 & 0 \\
                    0 & -1 & 0 & 0 \\
                    0 & 0 & -1 & 0 \\
                    0 & 0 & 0 & -1
                    \end{pmatrix}
                    \begin{pmatrix}
                    \gamma & -\beta \gamma & 0 & 0 \\
                    -\beta \gamma & \gamma & 0 & 0 \\
                    0 & 0 & 1 & 0 \\
                    0 & 0 & 0 & 1
                    \end{pmatrix} \\
                    &= \begin{pmatrix}
                    \gamma & +\beta \gamma & 0 & 0 \\
                    -\beta \gamma & -\gamma & 0 & 0 \\
                    0 & 0 & -1 & 0 \\
                    0 & 0 & 0 & -1
                    \end{pmatrix}
                    \begin{pmatrix}
                    \gamma & -\beta \gamma & 0 & 0 \\
                    -\beta \gamma & \gamma & 0 & 0 \\
                    0 & 0 & 1 & 0 \\
                    0 & 0 & 0 & 1  
                    \end{pmatrix} \\
                    &= \begin{pmatrix}
                    1 & 0 & 0 & 0 \\
                    0 & -1 & 0 & 0 \\
                    0 & 0 & -1 & 0 \\
                    0 & 0 & 0 & -1
                    \end{pmatrix} = g \, 
            \end{align*}
            wobei wir $\gamma^2 = 1 - \beta^2$ nutzen.

            \item  $\text{sgn} \Lambda^0_{\phantom{0} 0} = 1$

            Das stimmt offensichtlich, weil $\Lambda^0_{\phantom{0} 0} = \gamma > 0$ immer gilt.

            \item  $\det \Lambda = 1$ 
            \begin{align*}
                \det \Lambda &= \det \begin{pmatrix}
                    \gamma & -\beta \gamma & 0 & 0 \\
                    -\beta \gamma & \gamma & 0 & 0 \\
                    0 & 0 & 1 & 0 \\
                    0 & 0 & 0 & 1 \\
                \end{pmatrix} \\
                &= 1 \cdot \begin{vmatrix}
                    \gamma & -\beta \gamma & 0 \\
                    -\beta \gamma & \gamma & 0 \\
                    0 & 0 & 1 \\
                    \end{vmatrix} \\
                &= \gamma^2 - \beta^2 \gamma^2 = 1 \,.
            \end{align*}
        \end{enumerate}
        \textit{2 Punkte}
    }
    \item Drücken sie \eqref{eq:BoostsMatrix} durch die Rapidität $\eta = \artanh\left(\beta\right)$ aus. Welchen Vorteil kann es haben mit $\eta$ zu rechnen?
    \solution{Ein möglicher Weg wäre: 
        \begin{align*}
            \beta           &= \tanh(\eta)\\
            \gamma          &= \dfrac{1}{\sqrt{1 - \tanh^2(\eta)}}\\
                            &= \dfrac{1}{\sqrt{ \dfrac{\cosh^2(\eta)}{\cosh^2(\eta)} - \dfrac{\sinh^2(\eta)}{\cosh^2(\eta)}}} \, \text{mit}\, \cosh^2(\eta) - \sinh^2(\eta) = 1\\
                            &= \cosh(\eta) \\
            \Rightarrow  \beta \gamma   &= \tanh(\eta)\cosh(\eta) = \sinh(\eta)
        \end{align*}
        Zusammen gilt:
        \begin{align*}
               \Lambda &= \begin{pmatrix}
                \cosh(\eta) & -\sinh(\eta) & 0 & 0 \\
                -\sinh(\eta) & \cosh(\eta) & 0 & 0 \\
                0 & 0 & 1 & 0 \\
                0 & 0 & 0 & 1 \\
            \end{pmatrix} \\
        \end{align*}
        Die Rapidität geht über den gesamten Wertebereich $\eta \in \left(-\infty, \infty\right)$, was in­tu­i­tiver sein kann als $v \in \left(-c, c\right)$.
        Außderdem ist $\eta$ additiv $\eta_\text{ges} = \eta_1 + \eta_2$\,.
        Es muss nicht die Formel für die relativistische Geschwindigkeitaddition verwendet werden.\\
        \textit{1 Punkt}
        }
        \item Für einen kovarianten Vektor $x^{\mu}$ gilt unter einer LT $x^{\mu} \rightarrow x'^{\mu} = \Lambda_{\phantom{\nu} \nu}^\mu x^{\nu}\,.$
        Bestimmen Sie das allgemein Transformationverhalten eines kontravarianten Vektors $x_{\mu}$.
        \solution{
           $x_{\mu} \rightarrow x'_{\mu} = g_{\mu \nu} x'^{\nu} = g_{\mu \nu} \Lambda_{\phantom{\rho} \rho}^\nu x^{\rho} = \Lambda_{\mu \rho} x^{\rho} = \Lambda_{ \mu}^{\phantom{\nu} \nu} g_{\nu \rho} x^\rho = \Lambda_{ \mu}^{\phantom{\nu} \nu} x_\nu$ \\
           \textit{0,5 Punkte}
           }
        \item Bestimmen Sie das Transformationenverhalten der kovarianten Ableitung $\partial_\mu = \dfrac{\partial}{\partial x^\mu}$.\\
        \textit{Hinweis: Nutzen Sie, dass eine LT auch als $\dfrac{\partial x^\beta}{\partial x'^\alpha} = \Lambda^{\phantom{\beta} \beta}_\alpha$ geschrieben werden kann.}
        \solution{
            \begin{align}
                \dfrac{\partial}{\partial x^\mu} \rightarrow \dfrac{\partial}{\partial x'^\mu} = \dfrac{\partial x^\nu}{\partial x'^\mu}\dfrac{1}{\partial x^\nu} = \Lambda^{\phantom{\nu} \nu}_\mu \dfrac{1}{\partial x^\nu} = \Lambda^{\phantom{\nu} \nu}_\mu \partial_\nu\\
            \end{align}
            Die kovariante Ableitung transformiert wie eine kontravarianter Vektor.
            \textit{1 Punkt}
        }
        \item Zeigen Sie, dass eine Größe $x^\mu x_\mu$ tatsächlich invariant unter einer LT $x^\mu \rightarrow x'^\mu$ ist.
        \solution{
            \begin{align}
                x^\mu x_\mu = g_{\mu \nu} x^\mu x^\nu \rightarrow g_{\mu \nu} \Lambda_{\phantom{\rho} \rho}^\mu x^{\rho}  \Lambda_{\phantom{\sigma} \sigma}^\nu x^{\sigma} = \Lambda_{\phantom{\rho} \rho}^\mu g_{\mu \nu} \Lambda_{\phantom{\sigma} \sigma}^\nu  x^{\rho} x^{\sigma} = g_{\sigma \rho} x^{\rho} x^{\sigma} = x^{\rho} x_{\rho} = x^{\mu} x_{\mu}
            \end{align}
            \textit{0,5 Punkte}
        }

\end{enumerate}

\exercise{Gamma-Matrizen (4 Punkte)}

Sie haben Gamma-Matrizen betreits im Zuge der Dirac-Gleichung kennengelernt, wo eingesetzt wurden, um die verschiedenen Komponenten der Gleichung aneinander zu koppeln.
Diese $4 \times 4$-Matrizen treten, aber auch an anderer Stelle auf. 
So kann durch diese zum Beispiel ein konjugierter Spinor als $\bar{\psi} = \psi^\dagger \gamma^0$ geschrieben werden.
Ihr Verhalten wird durch den Antikommuator
\begin{equation}
    \label{eq:AntiKom}
    \left\{\gamma^\mu, \gamma^\nu \right\} = 2 g^{\mu \nu} \mathbb{1}_{4 \times 4}
\end{equation}
Bestimmt. 
$g^{\mu \nu}$ ist die Minkowskimetrik des flachen Raumes und $\gamma^\mu $ ein Vektor mit vier Dimensionen dessen Elemente die Gamma-Matrixen sind.
Die Wahl von $\gamma^\mu$ ist nicht eindeutig.
Eine mögliche Wahl ist die sogennate Dirac-Darstellung
\begin{align}
    \label{Gamma}
    \gamma^0 =   \begin{pmatrix}
                \mathbb{1}_{2 \times 2} & 0 \\
                0 & \text{-}\mathbb{1}_{2 \times 2} \\
    \end{pmatrix}\,, \qquad
    \gamma^\text{i} =   \begin{pmatrix}
        0 & \sigma^i \\
       \text{-} \sigma^i & 0 \\
    \end{pmatrix}
\end{align} 
mit $i = 1, 2, 3$ und den $2\times2$ Pauli-Matrixen $\sigma^i$.
\begin{enumerate}
    \item Zeigen sie durch eine explizite Rechnung, dass die Gamma-Matrizen \eqref{eq:AntiKom} erfüllen.
    \solution{
    Eine Fallunterscheindung
    \begin{enumerate}
        \item $\mu = \nu = 0$
        \begin{align}    
            \left\{ \gamma^0, \gamma^0\right\} = 2 
            \begin{pmatrix}
            \mathbb{1} & 0 \\
            0       & \text{-}\mathbb{1} \\
            \end{pmatrix}
            \begin{pmatrix}
                \mathbb{1} & 0 \\
                0       & \text{-}\mathbb{1} \\
            \end{pmatrix}  
            = 2 
            \begin{pmatrix}
                \mathbb{1} & 0 \\
                0       & \mathbb{1} \\
            \end{pmatrix}  
        \end{align}
        \item $\mu = 0, \nu = i$
        \begin{align}
            \left\{ \gamma^0, \gamma^i\right\} &=
            \begin{pmatrix}
            \mathbb{1} & 0 \\
            0       & \text{-}\mathbb{1} \\
            \end{pmatrix}
            \begin{pmatrix}
                0 & \sigma^i \\
                -\text{-}\sigma^i & 0 \\
            \end{pmatrix} 
            +
            \begin{pmatrix}
                0 & \sigma^i \\
                -\text{-}\sigma^i & 0 \\
            \end{pmatrix} 
            \begin{pmatrix}
                \mathbb{1} & 0 \\
                0       & \text{-}\mathbb{1} \\
            \end{pmatrix}
            &=    \begin{pmatrix}
                0 & \sigma^i \\
                \sigma^i & 0 \\
            \end{pmatrix}
            +
            \begin{pmatrix}
                0 & \text{-}\sigma^i \\
                \text{-}\sigma^i & 0 \\
            \end{pmatrix} = 0
        \end{align}
        \item $\mu = i, \nu = j$
        \begin{align}
            \left\{ \gamma^i, \gamma^j\right\} &= \begin{pmatrix}
                0 & \sigma^i \\
                \text{-}\sigma^i & 0 \\
            \end{pmatrix}
            \begin{pmatrix}
                0 & \sigma^j \\
                \text{-}\sigma^j & 0 \\
            \end{pmatrix} +
            \begin{pmatrix}
                0 & \sigma^j \\
                \text{-}\sigma^j & 0 \\
            \end{pmatrix}
            \begin{pmatrix}
                0 & \sigma^i \\
                \text{-}\sigma^i & 0 \\
            \end{pmatrix}\\
             &= \begin{pmatrix}
                -\sigma^i \sigma^j& 0 \\
                0 & -\sigma^i \sigma^j \\
            \end{pmatrix} +
            \begin{pmatrix}
                -\sigma^j \sigma^j& 0 \\
                0 & -\sigma^j \sigma^i \\
            \end{pmatrix}\\
              &= \begin{pmatrix}
                \text{-} \left\{ \sigma^i, \sigma^i \right\}& 0 \\
                0 & \text{-}\left\{ \sigma^i, \sigma^i \right\} \\
            \end{pmatrix} = \text{-}2 \delta_{ij}
            \begin{pmatrix}
                \mathbb{1} 0 \\
                0 & \mathbb{1} \\
            \end{pmatrix} = 2 g_{ij} \mathbb{1}_{4 \times 4}
         \end{align}
        \end{enumerate}
        \textit{2 Punkte}}
         \item Zeigen Sie ohne explizites Einsezten $\gamma^\mu \gamma_\mu = 4 \cdot \mathbb{1}_{4 \times 4}$ und $\gamma^\mu \gamma^\nu \gamma_\mu = \text{-} 2 \gamma^\nu $.
         \solution{
            \begin{align*}
                \gamma^\mu \gamma_\mu &= g_{\mu \nu} \gamma^\mu \gamma^\nu \\
                &= \frac{1}{2} \left( g_{\mu \nu} \gamma^\mu \gamma^\nu + g_{\mu \nu} \gamma^\mu \gamma^\nu \right) 
                \quad &\text{ (Benenne um } \mu \rightarrow \nu \text{ im rechten Term)} \\
                &= \frac{1}{2} \left( g_{\mu \nu} \gamma^\mu \gamma^\nu + g_{\nu \mu} \gamma^\nu \gamma^\mu \right) \\
                &= \frac{1}{2} \left( g_{\mu \nu} \gamma^\mu \gamma^\nu + g_{\mu \nu} \gamma^\nu \gamma^\mu \right) \\
                &= \frac{1}{2} g_{\mu \nu} \cdot 2 g^{\mu \nu} \, 1_{4 \times 4} \\
                &= g_{\mu \nu} g^{\mu \nu} \, 1_{4 \times 4} = 4 \cdot 1_{4 \times 4}.
            \end{align*}
            \begin{align*}
                \gamma^\mu \gamma^\nu \gamma_\mu &= \gamma^\mu \left( \gamma^\nu \gamma_\mu + \gamma_\mu \gamma^\nu - \gamma_\mu \gamma^\nu \right) \\
                &= \gamma^\mu \left( 2 \delta^\nu_\mu \cdot \mathbb{1}_{4 \times 4} - \gamma_\mu \gamma^\nu \right) 
                \quad &\text{(mit } \gamma^\mu \gamma_\mu = 4 \cdot \mathbb{1}_{4 \times 4}\text{)} \\
                &= 2 \gamma^\nu \cdot \mathbb{1}_{4 \times 4} - 4 \gamma^\nu \cdot \mathbb{1}_{4 \times 4} \\
                &= -2 \gamma^\mu \cdot \mathbb{1}_{4 \times 4}.
            \end{align*}
            \textit{1 Punkte}
            }
            \item Zeigen sie zusätzlich $(\gamma^\mu)^\dagger = \gamma^0 \gamma^\mu \gamma^0$.
            \solution{
                Da $\gamma^0$ reell und symmetrisch ist gilt offensichtlich $(\gamma^0)^\dagger = \gamma^0$ \\
                $\gamma^i$ ist entweder antisymmetrisch oder imaginär $(\gamma^i)^\dagger = \text{-}\gamma^i$\\
                \begin{align}
                    (\gamma^0)^\dagger &= \gamma^0 \gamma^0 \gamma^0 \quad \text{und}\\
                    (\gamma^i)^\dagger &= \text{-}\gamma^i = - \gamma^i \gamma^0 \gamma^0 = \gamma^0 \gamma^i \gamma^0
                \end{align}
                \textit{0,5 Punkte}}
            \item Es kann eine weitere Gamma-Matrix $\gamma^5 = i \gamma^0 \gamma^1 \gamma^2 \gamma^3$ definiert werden.\\
            Zeigen Sie, dass $\left\{\gamma^\mu, \gamma^5\right\} = 0$ gilt.
            \solution{
                \begin{align*}
                     \gamma^0 \gamma^5 &= i \gamma^0 \gamma^0 \gamma^1 \gamma^2 \gamma^3 
                    = -i \gamma^0 \gamma^1 \gamma^2 \gamma^3 \gamma^0 = -\gamma^5 \gamma^0. \\
                     \gamma^i \gamma^5 &= i \gamma^i \gamma^0 \gamma^1 \gamma^2 \gamma^3 = i  \gamma^0 \gamma^1 \gamma^2 \gamma^3 \gamma^i 
                    = -\gamma^5 \gamma^i \quad \text{(mit } i = 1, 2, 3) 
                \end{align*}
                \textit{0,5 Punkte}}
\end{enumerate}

\end{document}